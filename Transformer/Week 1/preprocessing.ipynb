{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processing completed successfully!\n",
      "Processed 14946 training samples and 2638 test samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, data_dir: str = \"./Data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.setup_logging()\n",
    "        \n",
    "        if not self.data_dir.exists():\n",
    "            raise FileNotFoundError(f\"Data directory not found: {self.data_dir}\")\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        log_dir = Path(\"logs\")\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            filename=log_dir / f\"math_qa_processor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\",\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "    \n",
    "    def verify_files_exist(self, file_names: List[str]) -> bool:\n",
    "        for file_name in file_names:\n",
    "            file_path = self.data_dir / file_name\n",
    "            if not file_path.exists():\n",
    "                logging.error(f\"Required file not found: {file_path}\")\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    def load_and_process_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Load and process data, returning separate training and testing DataFrames\"\"\"\n",
    "        train_files = ['main_train.csv', 'socratic_train.csv']\n",
    "        test_files = ['main_test.csv', 'socratic_test.csv']\n",
    "        \n",
    "        # Verify all files exist\n",
    "        all_files = train_files + test_files\n",
    "        if not self.verify_files_exist(all_files):\n",
    "            raise FileNotFoundError(\"One or more required files are missing\")\n",
    "            \n",
    "        train_dfs = []\n",
    "        test_dfs = []\n",
    "        \n",
    "        # Process training files\n",
    "        for file_name in train_files:\n",
    "            try:\n",
    "                df = pd.read_csv(self.data_dir / file_name)\n",
    "                logging.info(f\"Successfully loaded training file: {file_name}\")\n",
    "                \n",
    "                if not all(col in df.columns for col in ['question', 'answer']):\n",
    "                    logging.error(f\"Missing required columns in {file_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                df['question'] = df['question'].str.strip()\n",
    "                df['answer'] = df['answer'].str.strip()\n",
    "                train_dfs.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Process test files\n",
    "        for file_name in test_files:\n",
    "            try:\n",
    "                df = pd.read_csv(self.data_dir / file_name)\n",
    "                logging.info(f\"Successfully loaded test file: {file_name}\")\n",
    "                \n",
    "                if not all(col in df.columns for col in ['question', 'answer']):\n",
    "                    logging.error(f\"Missing required columns in {file_name}\")\n",
    "                    continue\n",
    "                    \n",
    "                df['question'] = df['question'].str.strip()\n",
    "                df['answer'] = df['answer'].str.strip()\n",
    "                test_dfs.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file_name}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not train_dfs or not test_dfs:\n",
    "            raise ValueError(\"No valid data files were processed\")\n",
    "            \n",
    "        train_data = pd.concat(train_dfs, ignore_index=True)\n",
    "        test_data = pd.concat(test_dfs, ignore_index=True)\n",
    "        \n",
    "        return train_data, test_data\n",
    "    \n",
    "    def create_training_config(self, train_data: pd.DataFrame, test_data: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Create training configuration with statistics from both datasets\"\"\"\n",
    "        data_stats = {\n",
    "            \"train_samples\": int(len(train_data)),\n",
    "            \"test_samples\": int(len(test_data)),\n",
    "            \"total_samples\": int(len(train_data) + len(test_data)),\n",
    "            \"max_question_length\": int(max(\n",
    "                train_data['question'].str.len().max(),\n",
    "                test_data['question'].str.len().max()\n",
    "            )),\n",
    "            \"avg_question_length\": float(pd.concat([\n",
    "                train_data['question'].str.len(),\n",
    "                test_data['question'].str.len()\n",
    "            ]).mean()),\n",
    "            \"unique_questions\": int(pd.concat([\n",
    "                train_data['question'],\n",
    "                test_data['question']\n",
    "            ]).nunique())\n",
    "        }\n",
    "        \n",
    "        config = {\n",
    "            \"model_name\": \"bert-base-uncased\",\n",
    "            \"batch_size\": 32,\n",
    "            \"learning_rate\": float(2e-5),\n",
    "            \"num_epochs\": 3,\n",
    "            \"max_sequence_length\": int(min(128, data_stats['max_question_length'])),\n",
    "            \"data_statistics\": data_stats,\n",
    "            \"files\": {\n",
    "                \"train\": str(Path(\"processed_data/train.csv\")),\n",
    "                \"test\": str(Path(\"processed_data/test.csv\"))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return config\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize preprocessor\n",
    "        preprocessor = DataPreprocessor()\n",
    "        \n",
    "        # Load and process data\n",
    "        train_data, test_data = preprocessor.load_and_process_data()\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = Path(\"processed_data\")\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save processed data\n",
    "        train_data.to_csv(output_dir / \"train.csv\", index=False)\n",
    "        test_data.to_csv(output_dir / \"test.csv\", index=False)\n",
    "        \n",
    "        # Create and save configuration\n",
    "        config = preprocessor.create_training_config(train_data, test_data)\n",
    "        with open(output_dir / \"training_config.json\", 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        logging.info(\"Data processing completed successfully\")\n",
    "        print(\"Data processing completed successfully!\")\n",
    "        print(f\"Processed {len(train_data)} training samples and {len(test_data)} test samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in main process: {str(e)}\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
